{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7addb",
   "metadata": {},
   "source": [
    "## LUISS - Project 1: The Librarian from Alexandria - Group 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12029695",
   "metadata": {},
   "source": [
    "Gabriele De Ieso, Denise Di Franza e Alessia Tonicello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c3f7c",
   "metadata": {},
   "source": [
    "##\n",
    "Objective \n",
    "As the newly appointed librarian of the Great Library of Alexandria, your task is to classify ancient digitized texts based on their fonts. This project requires developing a neural network \n",
    "model capable of accurately categorizing different writing styles to streamline the digital archiving process. \n",
    "\n",
    "\n",
    "Dataset Features:\n",
    "* 1256 scanned historical texts and their respective font used. \n",
    "* Documents originate from various years and contain different writing styles and fonts. \n",
    "* Requires cleaning, augmentation, and structured labeling. \n",
    "\n",
    "\n",
    "\n",
    "Assignment \n",
    "1. Perform Exploratory Data Analysis (EDA) \n",
    "\n",
    "* Understand the dataset structure. \n",
    "* Implement preprocessing steps (grayscale conversion, binarization, noise reduction). \n",
    "\n",
    "2. Data Augmentation \n",
    "\n",
    "* Apply transformation techniques to enrich training data.\n",
    "\n",
    "\n",
    "3. Model Development \n",
    "\n",
    "* Define and optimize a Neural Network topology for classification. \n",
    "* Test different architectures to determine the most effective model. \n",
    "\n",
    "\n",
    "4. Evaluation & Insights \n",
    "\n",
    "* Assess performance using appropriate classification metrics. \n",
    "* Provide a detailed report explaining model choices and key findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb282a9",
   "metadata": {},
   "source": [
    "# PHASE 0: IMPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879d379",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "In this code block, we import all the necessary libraries and modules required for the project. Here's a breakdown of what each part does:\n",
    "\n",
    "---\n",
    "\n",
    "### Standard Libraries\n",
    "- `os`, `random`, and `pathlib.Path` are used for file system navigation, random operations, and path handling.\n",
    "- `time` is used to measure execution time and track performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Manipulation and Scientific Computing\n",
    "- `numpy` and `pandas` provide powerful tools for numerical operations and structured data manipulation.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Visualization\n",
    "- `matplotlib.pyplot` and `matplotlib.gridspec` are used to create plots and define complex grid-based layouts.\n",
    "- `seaborn` adds a high-level interface for statistical graphics.\n",
    "- `tqdm.auto.tqdm` is used to display progress bars for loops, especially helpful during model training or data processing.\n",
    "- `IPython.display.display` and `HTML` are used to improve notebook visualization and ensure that output areas dynamically adjust height when displaying large content.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Processing\n",
    "- `PIL.Image` and `ImageFile` handle image loading and manipulation. We also disable the pixel size limit with `Image.MAX_IMAGE_PIXELS = None` to safely work with very large images.\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Core Libraries\n",
    "- `torch`, `torch.nn`, and `torch.optim` form the core of the PyTorch framework, providing tensor operations, neural network components, and optimization algorithms.\n",
    "- `torch.nn.functional` is used for functions like activation functions and loss computations that are not classes.\n",
    "- `torch.utils.data.Dataset`, `DataLoader`, `Subset`, and `random_split` are essential for data handling, batching, and creating custom datasets.\n",
    "\n",
    "- `torchvision.transforms` includes tools for data preprocessing and augmentation.\n",
    "- `torchvision.models` provides access to pre-trained models, such as `mobilenet_v2`, which we use for transfer learning.\n",
    "- `torch.cuda.amp.autocast` and `GradScaler` enable mixed precision training, speeding up computations on compatible hardware.\n",
    "- `torch.optim.lr_scheduler.ReduceLROnPlateau` automatically lowers the learning rate when validation metrics stagnate, improving training stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "- `sklearn.metrics.classification_report` and `confusion_matrix` help evaluate model performance on classification tasks.\n",
    "- `sklearn.model_selection.StratifiedKFold` performs stratified cross-validation to ensure that each fold preserves the class distribution.\n",
    "- `sklearn.utils.class_weight.compute_class_weight` is used to compute class weights for imbalanced datasets, ensuring that the model doesn't become biased toward more frequent classes.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Other Utilities\n",
    "- `tabulate` is a utility that formats tabular data nicely, especially useful for printing training results or summary statistics in a readable format.\n",
    "- `warnings` is used to suppress specific user warnings that may clutter notebook output.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "We’re setting up our environment by importing all the essential tools required for image preprocessing, model building, training, evaluation, and visualization. This foundational block ensures that our project runs efficiently, cleanly, and reproducibly across experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path  \n",
    "import time\n",
    "\n",
    "# Data Analysis & Visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Torch & TorchVision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# Mixed Precision & LR Scheduler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image, ImageFile\n",
    "Image.MAX_IMAGE_PIXELS = None  # Disabilita il controllo anti-OOM\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Utility\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.output_wrapper, .output { height:auto !important; max-height:100000px; }</style>\"))\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Tabulate (per tabelle ordinate)\n",
    "import tabulate as tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c94251",
   "metadata": {},
   "source": [
    "What we are doing and why:\n",
    "Setting the Random Seed:\n",
    "We set a fixed SEED value (42) for the random, numpy, and torch libraries. This ensures that every run of the code—especially data splits, weight initializations, and any stochastic operations—produces the same results. This is essential for debugging and validating model performance.\n",
    "\n",
    "GPU Determinism Settings:\n",
    "- If a CUDA-enabled GPU is available, we set the manual seed for all CUDA devices.\n",
    "- torch.backends.cudnn.deterministic = True ensures operations are deterministic (but might reduce performance).\n",
    "- torch.backends.cudnn.benchmark = False disables the auto-tuner, which is helpful for reproducibility but may slow down training slightly.\n",
    "\n",
    "Device Selection:  We detect whether a GPU is available and assign device accordingly, this allows us to later move our models and data to either CPU or GPU seamlessly.\n",
    "\n",
    "Output:\n",
    "The output will be a single line indicating the selected device, if a GPU is available, the output will be cuda, otherwise, it will be cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d844eb",
   "metadata": {},
   "source": [
    "In this step, we define the paths to our dataset components: the image directory and the metadata CSV file, these two elements are essential for structuring our data pipeline.\n",
    "We start by setting a base_dir, which points to the main folder containing both the images and the CSV file, from this base directory, we then construct the full path to:\n",
    "- **img_dir**, the subfolder containing all the image files—each image presumably representing a page with text in a specific font;\n",
    "- **csv_path**, the CSV file that links each image to its corresponding font label.\n",
    "Using Python’s Path object from the pathlib module makes path handling cleaner and more robust than relying on raw strings or concatenation, it also makes the code easier to maintain and modify later if we move the dataset or deploy the project elsewhere.\n",
    "At this point, we’re simply preparing variables we’ll need when loading and processing the data. No computation happens here yet, but it’s an essential foundational step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(r\"C:\\Users\\denis\\Downloads\\TheLibrarianFromAlexandria\")\n",
    "img_dir = base_dir / \"img\"\n",
    "csv_path = base_dir / \"pages.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d2e96",
   "metadata": {},
   "source": [
    "In this step, we load and verify the consistency of our dataset annotations. The pages.csv file, which associates each image file with a font label, is read into a DataFrame without headers, and we assign the column names \"image_name\" and \"font\" for clarity. To avoid potential ordering biases during training, we shuffle the dataset using random_state=42, ensuring that this operation is fully reproducible across runs.\n",
    "\n",
    "Next, we perform a thorough consistency check to ensure that all image paths listed in the CSV actually correspond to existing image files in the img folder. Since each image path in the CSV includes a \"img\\\\\" prefix that doesn’t match the folder structure, we remove this prefix when checking for file existence. If any image file is missing, we store it in a list, print a warning message showing how many are absent, and drop them from the dataset to prevent runtime errors later.\n",
    "\n",
    "To cross-validate this check, we apply an alternative method: we add a boolean exists column to the DataFrame by testing whether each image file is present using os.path.exists. Finally, we filter the rows where the image does not exist and print how many are missing according to this second method.\n",
    "\n",
    "In our case, both checks agree and the output confirms that all image files are present and accounted for zero missing images, this ensures that we can proceed with training and evaluation without encountering any broken links or file-not-found errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fec935",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(csv_path, header=None, names=[\"image_name\", \"font\"])\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    missing_images = []\n",
    "    for img_path in df['image_name']:\n",
    "        if not (img_dir / img_path.replace('img\\\\', '')).exists():\n",
    "            missing_images.append(img_path)\n",
    "    if missing_images:\n",
    "        print(f\"Attenzione: {len(missing_images)} immagini mancanti!\")\n",
    "        df = df[~df['image_name'].isin(missing_images)]\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    raise SystemExit(f\"pages non trovato: {e}\")\n",
    "\n",
    "df['exists'] = df['image_name'].apply(lambda x: os.path.exists(os.path.join(img_dir, os.path.basename(x))))\n",
    "missing_files = df[~df['exists']]\n",
    "print(f\"Missing images: {len(missing_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa47cb7",
   "metadata": {},
   "source": [
    "Before training our classification model, we need to convert the font names—which are currently strings—into numerical values that can be handled by PyTorch. This process, known as label encoding, is essential for transforming categorical variables into a format that neural networks can interpret.\n",
    "To achieve this, we first extract and sort all the unique font names found in the dataset, we then build a dictionary called font_to_label, where each font is mapped to a unique integer ID, starting from 0. This consistent ordering is important, especially for reproducibility and when interpreting the model’s output.\n",
    "Using the .map() function, we apply this dictionary to the \"font\" column, creating a new column called \"label\" that contains the numeric class associated with each image.\n",
    "To confirm the mapping was performed correctly, we print the full list of font-to-label associations and the output below shows a sample of this mapping:\n",
    "- Font: augustus -> Label: 0\n",
    "- Font: aureus -> Label: 1\n",
    "- Font: cicero -> Label: 2\n",
    "- Font: colosseum -> Label: 3\n",
    "- Font: consul -> Label: 4\n",
    "- Font: forum -> Label: 5\n",
    "- Font: laurel -> Label: 6\n",
    "- Font: roman -> Label: 7\n",
    "- Font: senatus -> Label: 8\n",
    "- Font: trajan -> Label: 9\n",
    "- Font: vesta -> Label: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bdbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_names = sorted(df['font'].unique())\n",
    "font_to_label   = {font: idx for idx, font in enumerate(font_names)}\n",
    "df['label'] = df['font'].map(font_to_label)\n",
    "print(\"Mapping of font to label:\")\n",
    "for font, label in font_to_label.items():\n",
    "    print(f\"Font: {font} -> Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09245da6",
   "metadata": {},
   "source": [
    "# PHASE 1: EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ce16d",
   "metadata": {},
   "source": [
    "To get a clearer understanding of our dataset after label encoding, we display the first 20 rows of the DataFrame. Each row corresponds to a single image sample, and the columns include:\n",
    "- **image_name**: the filename of the image,\n",
    "- **font**: the original font name (as a string),\n",
    "- **exists**: a boolean indicating whether the image file is present on disk (useful for debugging),\n",
    "- **label**: the integer class ID assigned through label encoding.\n",
    "This preview allows us to verify that the label encoding was correctly applied and that each image has an associated numerical label.\n",
    "\n",
    "We also print two key pieces of information:\n",
    "- **The number of unique fonts**, which corresponds to the number of classes the model will learn to predict (11)\n",
    "- **The total number of rows in the dataset**, which tells us how many samples are available for training and evaluation (1256)\n",
    "This sanity check ensures that our dataset is correctly structured before proceeding to create PyTorch datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58850dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(20))\n",
    "print(\" \")\n",
    "print(\"Number of unique fonts:\", len(font_to_label))\n",
    "print(\"Number of rows in the dataset:\", len(df))\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2373e0",
   "metadata": {},
   "source": [
    "Before feeding the data into our model, we perform a quick visual inspection of the font classes, in particular we group the dataset by the font column, and for each group, we display the first image associated with that font, this helps us get a qualitative understanding of the dataset's visual diversity and check if the images are correctly labeled.\n",
    "Using a 3x4 grid, we arrange the sample images so that each subplot displays one representative image per font with its name as the title and next, we check the dimensions of the images in the dataset. By extracting the width and height of each image, we calculate their average dimensions, this analysis shows that the image widths are significantly large (≈ 4777 px), while the heights are also quite substantial (≈ 4827 px). This could indicate that the images are high-resolution and may need to be resized or processed to match the input requirements of the model, particularly if it expects a standard image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 12))\n",
    "gs = gridspec.GridSpec(3, 4, figure=fig) \n",
    "for idx, (font, sub_df) in enumerate(df.groupby('font')):\n",
    "    ax = fig.add_subplot(gs[idx])\n",
    "    img_path = os.path.join(img_dir, sub_df.iloc[0]['image_name'].split('\\\\')[-1])\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(font, fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sizes = [Image.open(os.path.join(img_dir, f.split('\\\\')[-1])).size for f in df['image_name']]\n",
    "widths, heights = zip(*sizes)\n",
    "print(f\"Image dimensions: Avg W={np.mean(widths)}, Avg H={np.mean(heights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06123bc0",
   "metadata": {},
   "source": [
    "Here, what we've done is to first assess the distribution of the fonts in our dataset, in fact understanding how many samples exist for each font is crucial, as an imbalanced dataset can lead to a biased model. We start by calculating the number of occurrences of each font using the value_counts() method, which gives us the absolute count of images for each font, then, to better understand the relative importance of each font, we calculate the percentage of the total dataset that each font represents (this is done by dividing the count of each font by the total number of samples, and then multiplying by 100).\n",
    "\n",
    "After performing these calculations, we put the data into a table that shows both the absolute count and the percentage for each font, this provides a clear numerical overview of how the fonts are distributed across the dataset.\n",
    "Next, we visualize the distribution using a bar plot, in which, each bar represents a different font, with the length of the bar indicating how many samples belong to that font and to make it easier to read, we annotate each bar with both the count and percentage of samples for that font.\n",
    "\n",
    "The output of this analysis includes two main results:\n",
    "- A table that shows the font counts and percentages;\n",
    "- A bar chart that visually presents the distribution of fonts, with counts and percentages labeled on each bar.\n",
    "This analysis helps us quickly identify any potential issues, such as whether some fonts are overrepresented or underrepresented, if any imbalances are found, we can take appropriate steps, such as adjusting the dataset, to ensure the model has a fair chance to learn from all fonts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_counts = df[\"font\"].value_counts()\n",
    "font_distribution = ((font_counts / font_counts.sum()) * 100).round(2)\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Count': font_counts,\n",
    "    'Percentage (%)': font_distribution\n",
    "})\n",
    "\n",
    "print(\"\\nDistribution for each font:\")\n",
    "print(distribution_df.to_string(formatters={'Percentage (%)': '{:.2f}%'.format}))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.countplot(\n",
    "    data=df, \n",
    "    y='font', \n",
    "    order=font_counts.index,\n",
    "    palette='viridis',\n",
    "    edgecolor='black'\n",
    ")\n",
    "for i, (value, name) in enumerate(zip(font_counts.values, font_counts.index)):\n",
    "    ax.text(\n",
    "        value + font_counts.max()*0.02,\n",
    "        i,\n",
    "        f'{value}\\n({font_distribution[name]}%)',  \n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=9,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.title('Font distribution in the dataset', fontsize=14, pad=20)\n",
    "plt.xlabel('Number of samples', fontsize=12)\n",
    "plt.ylabel('Font', fontsize=12)\n",
    "plt.xlim(0, font_counts.max() * 1.2)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645268d9",
   "metadata": {},
   "source": [
    "*Main Findings from EDA:*\n",
    "- Font Distribution: The dataset contains 12 distinct fonts, with varying frequencies. The top 3 fonts—aureus, cicero, and roman—make up a significant portion of the dataset. Aureus has the highest count at 142 images, representing approximately 11.31% of the total dataset, followed by cicero with 136 images (10.83%) and roman with 130 images (10.35%); the remaining fonts have fewer occurrences, with forum having the least, at 86 images (6.85%).\n",
    "- Image Dimensions: The images in the dataset are generally large, with an average width of 4777 pixels and an average height of 4827 pixels, this suggests that the images need resizing or adjustment for uniformity before feeding them into a model, as such large dimensions can lead to high computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = img_dir\n",
    "CSV_PATH = csv_path\n",
    "TARGET_SIZE = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74107135",
   "metadata": {},
   "source": [
    "# PHASES 1.1 PRE-PROCESSING + 1.2 DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f8b17",
   "metadata": {},
   "source": [
    "In this part of the analysis, we focus on preprocessing the images to enhance their quality for further text extraction, the main objective is to process the images so that text can be clearly identified, especially in cases where images may contain noisy or low-contrast areas:\n",
    "1. **Grayscale Conversion:**\n",
    "We start by converting the images to grayscale. This is a typical step in image preprocessing because color information is not necessary for text extraction, and converting to grayscale reduces the complexity of the image data.\n",
    "\n",
    "2. **Contrast Enhancement using CLAHE:**\n",
    "Next, we apply **Contrast Limited Adaptive Histogram Equalization (CLAHE)**, which improves the local contrast of the image, in fact CLAHE is particularly useful for images with uneven lighting or areas of the image where text may be hard to distinguish due to poor contrast.It works by dividing the image into small tiles and adjusting the contrast in each tile individually, ensuring that no area becomes overly bright or dark; by applying CLAHE, we expect to see improved visibility of text, particularly in areas where the contrast between the text and the background is weak.\n",
    "\n",
    "3. **Handling Double-Page Images:**\n",
    "If an image represents a double-page spread (i.e., it has a wide aspect ratio), we split it into two separate images: one for the left page and one for the right page; this allows us to treat each page individually for better text extraction and reduces the risk of missing text that might span across the middle of the spread. We rely on the image’s width-to-height ratio to determine whether it needs splitting, if the width exceeds the threshold (set at 1.2 times the height), we split the image in half.\n",
    "\n",
    "4. **Text Patch Extraction:**\n",
    "For each page (whether split or not), we apply binary thresholding to the image in order to isolate text. **Adaptive thresholding** is used, meaning the threshold value for binarization is computed for each pixel based on its local neighborhood, this is beneficial for images with varying lighting conditions.\n",
    "We extract small patches of the image (e.g., 255x255 pixels) using a sliding window approach, the goal is to identify areas that likely contain text and we do this by checking the fraction of white pixels in each patch, which helps distinguish text areas from the background. If a patch has a sufficient amount of text (as defined by a threshold), it is saved as a text patch and if the number of detected patches is too low, we fall back to extracting a patch from the center of the image, which is likely to contain text as well.\n",
    "\n",
    "### **Expected Results:**\n",
    "By the end of this process, we expect to have:\n",
    "* **Grayscale images** with enhanced contrast, making text more visible and easier to extract;\n",
    "* **Split images** for double-page spreads, ensuring that each page is treated individually;\n",
    "* **Text patches** that contain regions of interest, so where the text is present, and these patches will be used in subsequent steps for optical character recognition (OCR) or other forms of text analysis.\n",
    "\n",
    "In summary, this preprocessing workflow improves the quality of images, especially those with low contrast or double-page spreads, and extracts small, relevant areas of text for further analysis, the expected output is a set of cleaned and processed image patches that contain text, ready for the next stage in the image processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(img):\n",
    "    return img.convert('L')\n",
    "\n",
    "def apply_CLAHE(pil_img: Image.Image, clipLimit: float = 2.0, tileGridSize: [int, int] = (8, 8)) -> Image.Image:\n",
    "    gray = np.array(pil_img.convert('L'))\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    enhanced = clahe.apply(gray)\n",
    "    return Image.fromarray(enhanced)\n",
    "\n",
    "\n",
    "def split_double_page(pil_img, ratio_thr=1.2):\n",
    "    w, h = pil_img.size\n",
    "    if w / h > ratio_thr:\n",
    "        mid = w // 2\n",
    "        return [pil_img.crop((0, 0, mid, h)), pil_img.crop((mid, 0, w, h))]\n",
    "    else:\n",
    "        return [pil_img]\n",
    "\n",
    "def extract_text_patches(image,\n",
    "                         patch_size=(255, 255),\n",
    "                         thresh_method='adaptive',\n",
    "                         text_area_range=(0.2, 0.8),\n",
    "                         denoise=True,\n",
    "                         use_clahe=False,\n",
    "                         min_patches=1,\n",
    "                         include_center=True):\n",
    "\n",
    "    all_patches = []\n",
    "    pages = split_double_page(image)\n",
    "\n",
    "    for page in pages:\n",
    "        if use_clahe:\n",
    "            gray = np.array(apply_CLAHE(page))\n",
    "        else:\n",
    "            gray = np.array(page.convert('L'))\n",
    "        \n",
    "        if thresh_method == 'adaptive':\n",
    "            bin_img = cv2.adaptiveThreshold(\n",
    "                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                cv2.THRESH_BINARY_INV, blockSize=21, C=5\n",
    "            )\n",
    "        else:\n",
    "            _, bin_img = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        \n",
    "        H, W = bin_img.shape\n",
    "        ph, pw = patch_size\n",
    "        stride_h, stride_w = ph // 2, pw // 2\n",
    "\n",
    "        for y in range(0, H - ph + 1, stride_h):\n",
    "            for x in range(0, W - pw + 1, stride_w):\n",
    "                sub = bin_img[y:y+ph, x:x+pw]\n",
    "                frac = sub.sum() / (255 * ph * pw)\n",
    "                if text_area_range[0] < frac < text_area_range[1]:\n",
    "                    all_patches.append(page.crop((x, y, x+pw, y+ph)))\n",
    "        \n",
    "        if include_center and len(all_patches) < min_patches:\n",
    "            cx, cy = W // 2, H // 2\n",
    "            hw, hh = pw // 2, ph // 2\n",
    "            c_patch = page.crop((cx-hw, cy-hh, cx+hw, cy+hh))\n",
    "            all_patches.append(c_patch)\n",
    "    \n",
    "    return all_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1d781",
   "metadata": {},
   "source": [
    "In this part of the code, we define the transformations applied to the images during the training and validation phases. These transformations are crucial for preparing the dataset, augmenting it for the training process, and normalizing it for better performance in machine learning models.\n",
    "1. **Resizing:**\n",
    "Both training and validation sets are resized to a target size of **128x128 pixels**, this standardizes the images, ensuring that they all have the same dimensions, which is essential for feeding them into neural networks that require a fixed input size.\n",
    "\n",
    "2. **Grayscale Conversion:**\n",
    "The images are converted to grayscale, with a single channel, this simplifies the image and removes color information, focusing on the intensity of light in the image, and this is often sufficient for tasks such as text recognition, where color information may not be necessary.\n",
    "\n",
    "3. **Data Augmentation (Training Set Only):**\n",
    "   * **Random Horizontal Flip:** The images in the training set are randomly flipped horizontally with a probability of 50%, this helps the model generalize better by providing more diverse variations of the same image.\n",
    "   * **Random Affine Transformation:** Random affine transformations are applied, including rotation (up to 10 degrees), translation (shift up to 10% of the image size), and scaling (up to ±10%); these transformations simulate slight variations in the positioning and orientation of the objects in the images, which is useful to make the model more robust to real-world variations.\n",
    "   * **Random Perspective:** A random perspective distortion is applied with a scale of 0.2, meaning the image is distorted slightly as if viewed from different angles, this helps improve the model’s ability to recognize objects from various perspectives.\n",
    "   * **Random Erasing (Optional):** There is an option to apply **Random Erasing**, which randomly erases parts of the image, this can simulate occlusions and forces the model to focus on the remaining visible parts of the image, however, this is commented out for now and can be activated if necessary for further augmentation.\n",
    "\n",
    "4. **Tensor Conversion and Normalization:**\n",
    "   * The **ToTensor** transformation converts the image into a tensor, which is a format that deep learning models in PyTorch can work with.\n",
    "   * **Normalization:** The images are then normalized with a mean and standard deviation of **0.5** ensuring that the pixel values of the images are scaled between -1 and 1, which helps the neural network train more efficiently by stabilizing gradients.\n",
    "\n",
    "5. **Validation Transformations:**\n",
    "   * The validation set undergoes fewer transformations than the training set, only resizing, grayscale conversion, and normalization are applied. This ensures that the validation images are consistent with the format expected by the model, but without augmentation, as the model is not being trained on the validation set.\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "* **Training Set:** The images will be augmented in various ways (flipping, rotation, scaling, etc.) to create diversity in the dataset, this helps prevent overfitting and allows the model to generalize better to new, unseen data.\n",
    "* **Validation Set:** The validation images are simply resized, converted to grayscale, and normalized, ensuring that the model is evaluated on consistent, unaltered data.\n",
    "\n",
    "By applying these transformations, the model should become more robust to variations in input images and able to generalize better to unseen data. The normalization step ensures that the model receives data in a form that it can effectively process, while the augmentations for training allow it to learn from a wider variety of image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_SIZE = (128, 128)\n",
    "\n",
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.Grayscale(num_output_channels=1),  \n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9,1.1)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ])\n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e1d58",
   "metadata": {},
   "source": [
    "In this next cell, we manually test the extract_text_patches() function on a small subset of images to ensure that it works correctly before integrating it into the full dataset pipeline. Specifically, we select the first 5 images from the dataset, load them, and apply the patch extraction function, we then save a few extracted patches per image to verify their visual quality and content.\n",
    "\n",
    "We do this for several reasons:\n",
    "- To verify that the image paths in the dataframe are correct and accessible;\n",
    "- To inspect whether the patch extraction logic detects and crops meaningful areas of text;\n",
    "- To generate visual debug samples that allow us to assess the quality of patch selection.\n",
    "\n",
    "At the end of this cell, we expect the console to print:\n",
    "- The full path of each loaded image and a confirmation that the file exists.\n",
    "- The number of patches extracted per image.\n",
    "- Confirmation messages for each patch saved.\n",
    "\n",
    "Additionally, up to three image patches per sample image will be saved in the current working directory. These .png files can be inspected manually to confirm that they correspond to regions containing text — they will be named debug_idx{X}_patch{Y}.png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "test_indices = list(range(5))\n",
    "\n",
    "for idx in test_indices:\n",
    "    filename = os.path.basename(df.loc[idx, 'image_name'])\n",
    "    full_path = os.path.join(img_dir, filename)\n",
    "    print(f\"Uploading   {full_path}   → exists? {os.path.exists(full_path)}\")\n",
    "    img = Image.open(full_path).convert('RGB')\n",
    "    patches = extract_text_patches(\n",
    "        img,\n",
    "        include_center=True,\n",
    "        min_patches=1\n",
    "    )\n",
    "    print(f\"→ Per index {idx} found {len(patches)} patch\")\n",
    "    for j, p in enumerate(patches[:3]):\n",
    "        out_name = f\"debug_idx{idx}_patch{j}.png\"\n",
    "        p.save(out_name)\n",
    "        print(f\"   • {out_name} saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7169b30",
   "metadata": {},
   "source": [
    "# PHASE 1.3 DATASET CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f4c94",
   "metadata": {},
   "source": [
    "In this part of the code, we define a custom PyTorch dataset class called FontDataset, which is specifically designed for font classification tasks based on scanned document images. This dataset implementation allows the model to focus on localized regions of text, even in the presence of noise or double-page scans, and includes a robust mechanism for handling failed image loads or missing text regions.\n",
    "\n",
    "Key Features:\n",
    "**Image Loading and Preprocessing:**\n",
    "The dataset receives a pandas DataFrame (df) containing image filenames and labels, and a root directory (img_dir) where the images are stored.\n",
    "Each image is loaded using its filename and converted to RGB format and in case of load failure (e.g., corrupted file, missing image), a fallback image (a black dummy patch of the specified size) is returned, with a default label of -1.\n",
    "**Patch Extraction from Split Pages:**\n",
    "Each image may contain a double-page scan, to improve model focus, the image is first split into two single pages using the split_double_page utility.\n",
    "From each of these pages, relevant patches of text are extracted using extract_text_patches, this function looks for textual regions and returns sub-images of size patch_size that include the central part of the page and at least one valid patch.\n",
    "**Fallback Mechanism: Center Crop**\n",
    "If no valid patches are found (e.g., the image is blank, has no clear text region, or the text is cropped), the dataset performs a simple center crop of the original image to obtain a patch, this ensures that the model still receives input data and training can proceed without interruption.\n",
    "**Image Transformations (Optional):**\n",
    "If a transformation pipeline is provided (e.g., resizing, grayscale conversion, normalization), it is applied to the selected patch, otherwise, the raw patch (as a PIL image) is returned.\n",
    "**Label Assignment:**\n",
    "The label is retrieved from the DataFrame as an integer class ID and returned along with the image patch.\n",
    "**Data Visualization Method:**\n",
    "The class also includes a helper method visualize_samples() that allows users to view:\n",
    "- The original image associated with a given index.\n",
    "- The corresponding transformed image patch that would be used for training or evaluation.\n",
    "- The font name and class label associated with each image.\n",
    "This is useful for debugging the dataset construction process and verifying that the patch extraction and transformations are working as expected.\n",
    "\n",
    "**Expected Outcomes**\n",
    "Each call to __getitem__ will return a cropped or extracted patch representing the most informative portion of the original document, even in the case of corrupt or low-quality images, the fallback system ensures consistent input dimensions and label structure. During training and validation, the model receives properly preprocessed image data, increasing robustness and reducing the risk of learning from irrelevant background noise or inconsistent formatting.\n",
    "By using this dataset class, the training process becomes more stable, flexible, and focused on relevant features—especially important in font classification tasks where local textual texture and structure are key discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, patch_size=(512, 512)):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = df['image_name'].apply(lambda x: os.path.basename(x)).tolist()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_paths[idx])\n",
    "        try:\n",
    "            original_image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Fallback: immagine nera\n",
    "            dummy = Image.new('RGB', self.patch_size, color=(0,0,0))\n",
    "            if self.transform:\n",
    "                return self.transform(dummy), -1\n",
    "            return dummy, -1\n",
    "\n",
    "        patches = []\n",
    "        for page in split_double_page(original_image):\n",
    "            patches.extend(\n",
    "                extract_text_patches(\n",
    "                    page,\n",
    "                    patch_size=self.patch_size,\n",
    "                    include_center=True,\n",
    "                    min_patches=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if not patches:\n",
    "            img_patch = transforms.functional.center_crop(original_image, self.patch_size)\n",
    "        else:\n",
    "            img_patch = random.choice(patches)\n",
    "\n",
    "        if self.transform:\n",
    "            img_final = self.transform(img_patch)\n",
    "        else:\n",
    "            img_final = img_patch\n",
    "\n",
    "        label = int(self.df.iloc[idx]['label'])\n",
    "        return img_final, label\n",
    "\n",
    "    def visualize_samples(self, indices, title):\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i, idx in enumerate(indices):\n",
    "            img_t, label = self.__getitem__(idx)\n",
    "            font_name = self.df.iloc[idx]['font']\n",
    "\n",
    "            plt.subplot(2, len(indices), i + 1)\n",
    "            orig = Image.open(os.path.join(self.img_dir, self.image_paths[idx])).convert('RGB')\n",
    "            plt.imshow(orig)\n",
    "            plt.title(f\"Originale\\n{font_name}\", fontsize=8)\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, len(indices), i + 1 + len(indices))\n",
    "            if isinstance(img_t, torch.Tensor):\n",
    "                img_show = img_t.permute(1, 2, 0).numpy().squeeze()\n",
    "            else:\n",
    "                img_show = np.array(img_t)\n",
    "            plt.imshow(img_show, cmap='gray' if img_show.ndim==2 else None)\n",
    "            plt.title(f\"Trasformata\\n{label}\", fontsize=8)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14326337",
   "metadata": {},
   "source": [
    "# PHASE 1.4 SPLITTING DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c03273",
   "metadata": {},
   "source": [
    "In this section, we orchestrate the final steps of dataset preparation: stratified splitting, transformation assignment, and visual inspection of samples at three stages—full dataset, training set, and test set.\n",
    "\n",
    "First, we perform a stratified split using `StratifiedShuffleSplit` with one split, allocating 20% of the data to the test set and fixing the random seed for reproducibility. This ensures that each font class maintains its original proportion in both the training and test subsets. We then call `get_transforms()` to retrieve two transformation pipelines: one for training, which includes data augmentations, and a simpler one for validation/testing.\n",
    "\n",
    "Next, we instantiate three `FontDataset` objects. The first wraps the entire DataFrame so we can visualize five random examples from the complete dataset, giving an initial sense of the raw images and labels. We then create separate DataFrames for the training and test splits (resetting their indices for convenience) and instantiate two more datasets, each with its own transform: the training dataset receives the augmentation pipeline, while the test dataset receives only resizing and normalization.\n",
    "\n",
    "Finally, we call `visualize_samples()` on each dataset: first on the full dataset, then on the training set, and lastly on the test set. In each case, the method displays a grid of five images where the top row shows the original page crops and the bottom row shows the transformed patches that will actually be fed to the model. By comparing these visuals side by side, we can quickly verify that the split preserved class balance, that augmentations are applied only to the training data, and that normalization and resizing are correctly applied to the test data before model evaluation.\n",
    "\n",
    "The **output** of this section consists of three distinct visual inspection blocks:\n",
    "- \"Dataset examples complete\":\n",
    "A preview of five random samples from the full dataset before any transformations, this provides a baseline view of the raw data and confirms that the dataset is correctly loaded and indexed.\n",
    "- \"Training Set - Post\":\n",
    "A preview of five images from the training set after applying the training transformations, these images are expected to show a variety of augmentations (e.g., flipping, distortion, or perspective changes), verifying that data augmentation is active and correctly configured.\n",
    "- \"Test Set - Post\":\n",
    "A preview of five images from the test set after applying validation transformations, these images should appear more uniform and structured, since no augmentations are applied — only resizing, grayscaling, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, test_idx = next(sss.split(np.arange(len(df)), df['label']))\n",
    "train_transform, val_transform = get_transforms()\n",
    "full_dataset = FontDataset(df, IMG_DIR)\n",
    "sample_indices = np.random.choice(len(full_dataset), 5, replace=False)\n",
    "full_dataset.visualize_samples(sample_indices, \"Dataset examples complete\")\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "train_dataset = FontDataset(train_df, IMG_DIR, transform=train_transform)\n",
    "test_dataset = FontDataset(test_df, IMG_DIR, transform=val_transform)\n",
    "\n",
    "print(\"\\n TRAINING SET\")\n",
    "train_dataset.visualize_samples(range(5), \"Training Set - Post\")\n",
    "print(\"\\n TEST SET\")\n",
    "test_dataset.visualize_samples(range(5), \"Test Set - Post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4e76e",
   "metadata": {},
   "source": [
    "This section focuses on evaluating how our dataset is distributed across different splits. To achieve this, we use the function get_split_stats, which helps us analyze the composition of each subset by examining the frequency of different font classes.\n",
    "\n",
    "We start by extracting the relevant portion of the dataset using the indices provided for the split (training or test set) and once we have this subset, we determine the total number of observations within it. To better understand how each font is represented, we iterate through the unique font classes in the dataset and for each font, we calculate how many observations exist in the entire dataset and how many fall within the current split; additionally, we compute two key percentages: the proportion of the font within the full dataset and its representation within the split.\n",
    "\n",
    "This data is then organized into a structured format using pandas DataFrame, making it easy to visualize and interpret, also to enhance readability, the results are presented in a neatly formatted table using the tabulate library. Along with the table, we include clear section headers that highlight the total number of observations and the details of the split being analyzed.\n",
    "\n",
    "The expected output provides a snapshot of how well the dataset partitions maintain proportionality across font classes. This analysis ensures that our splits are balanced and that no particular font dominates the training or test set disproportionately, by reviewing these statistics, we gain confidence in the integrity of our dataset and its suitability for model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "def get_split_stats(full_df, split_indices, split_name):\n",
    "    split_df = full_df.iloc[split_indices]\n",
    "    total_samples = len(split_df)\n",
    "    \n",
    "    stats = []\n",
    "    for font in full_df['font'].unique():\n",
    "        font_total = len(full_df[full_df['font'] == font])\n",
    "        font_split = len(split_df[split_df['font'] == font])\n",
    "        \n",
    "        stats.append({\n",
    "            'Font': font,\n",
    "            'N° Observations': font_split,\n",
    "            '% x Font': f\"{(font_split / font_total * 100):.1f}%\",\n",
    "            '% x Split': f\"{(font_split / total_samples * 100):.1f}%\"\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{split_name.upper()} SET STATISTICS\")\n",
    "    print(f\"Total number of observations: {total_samples}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(tabulate(stats_df, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848fbf5",
   "metadata": {},
   "source": [
    "Once we have split the dataset and created the training subset, the next step is ensuring that our model accounts for class imbalances. Some font classes may be underrepresented in the dataset, leading the model to favor more frequent categories, so to mitigate this issue, we compute class weights that assign higher importance to less frequent classes, balancing their impact during training.\n",
    "\n",
    "We start by extracting the training subset from our main DataFrame using `train_idx`, then, we use `compute_class_weight` from `sklearn.utils.class_weight` to calculate weights inversely proportional to class frequencies. The `balanced` mode ensures that rarer fonts receive higher weights, making their contribution comparable to more common ones.\n",
    "\n",
    "With these weights computed, we establish a mapping from each font to its corresponding weight using a dictionary, this allows us to reference the weight of any given font efficiently; finally, we transform these weights into a PyTorch tensor, ensuring compatibility with our model. The tensor is ordered according to the predefined numerical labels, which should have been set earlier in the notebook.\n",
    "\n",
    "This approach guarantees that our model does not disproportionately favor dominant font classes, improving its ability to generalize across all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99afda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "classes = train_df['font'].unique()\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_df['font']\n",
    ")\n",
    "font2weight = {font: w for font, w in zip(classes, weights)}\n",
    "weight_list = [font2weight[f] for f in font_names]  \n",
    "class_weights = torch.tensor(weight_list, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0827a4f",
   "metadata": {},
   "source": [
    "This command calls the get_split_stats function to generate and display statistical insights for the training set, based on the previously defined logic, it will extract the subset of data corresponding to train_idx, calculate distribution percentages for each font class, and format the output neatly using tabulate.\n",
    "After execution, the output will include:\n",
    "- The total number of observations in the training set: 1004\n",
    "- A structured table showing how each font is represented, both within the full dataset and specifically in the training split (remembering that our objective was a more or less balanced distribution and around 80% for each font in the training set)\n",
    "- A well-formatted visual summary to quickly assess balance across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = get_split_stats(df, train_idx, \"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e2bd3",
   "metadata": {},
   "source": [
    "Just as we did for the training set, we now apply the same statistical analysis to the test set. By calling get_split_stats(df, test_idx, \"Test\"), we extract the relevant observations, compute the proportional distribution of each font, and present the results in a structured format. This ensures that we maintain balance across our dataset splits and allows us to verify that no particular class dominates disproportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37972e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = get_split_stats(df, test_idx, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760986ba",
   "metadata": {},
   "source": [
    "To ensure that our dataset split was executed correctly, we print a confirmation of the total number of samples. This allows us to verify that all observations have been accounted for and that there are no discrepancies between the sum of the training and test sets and the original dataset size.\n",
    "The output will show a breakdown of the number of samples assigned to training and test sets, confirming that their sum matches the total dataset size. This simple check reassures us that the stratified split was properly performed before proceeding with further data analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset training + dataset test: {len(train_idx)} + {len(test_idx)} = {len(train_idx)+len(test_idx)}\")\n",
    "print(f\"Dataset: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c345dad",
   "metadata": {},
   "source": [
    "## PHASE 1.5 DATA LOADERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb368fc",
   "metadata": {},
   "source": [
    "At this stage, we set up our data loaders, which will facilitate efficient batch processing during training and validation, choosing optimal parameters ensures that data is loaded quickly and minimizes memory overhead, especially when working with large datasets.\n",
    "\n",
    "We define the `batch_size` as 32, determining how many samples are processed in each batch. To balance computational resources, we calculate `num_workers` dynamically using `os.cpu_count()`, dividing by two to allocate a reasonable amount of parallel processing while avoiding excessive resource consumption. If the system's CPU count isn't available, we default to at least one worker. Additionally, we enable `pin_memory`, which accelerates data transfer to the GPU if CUDA is available.\n",
    "\n",
    "With these settings in place, we initialize the `DataLoader` instances:\n",
    "- For the training set, we enable `shuffle=True` to ensure the model sees diverse samples in each epoch, improving generalization;\n",
    "- For validation, we maintain `shuffle=False`, preserving a fixed order that keeps evaluation consistent across runs.\n",
    "\n",
    "To verify our configuration, we print the total number of batches in both loaders, this allows us to check that the data has been correctly partitioned and that the setup aligns with expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = os.cpu_count() // 2 or 1\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                      num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=num_workers, pin_memory=pin_memory)\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b8fbc",
   "metadata": {},
   "source": [
    "## PHASE 2 MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66ab35",
   "metadata": {},
   "source": [
    "**After the dataset import, exploration and manipulation, we can finally start building our model**\n",
    "\n",
    "(We have tried various models, but we leave you only the most important and significant ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e4504",
   "metadata": {},
   "source": [
    "# 1° Model: EnhancedFontCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958f720",
   "metadata": {},
   "source": [
    "We begin with our first deep learning model, EnhancedFontCNN, a convolutional neural network tailored for font classification. Its architecture is designed to extract meaningful visual features and classify different font styles with precision.\n",
    "\n",
    "The model’s feature extraction component consists of multiple convolutional layers (4), each followed by batch normalization and a ReLU activation function. These layers progressively refine the representation of input images, capturing intricate patterns. As we move deeper into the network, pooling operations reduce the dimensionality of the feature maps, enhancing efficiency without losing essential details. Finally, an adaptive average pooling layer standardizes the feature output before transitioning to classification.\n",
    "\n",
    "In the classification stage, the extracted features are flattened into a single vector and passed through fully connected layers. The combination of dropout, batch normalization, and non-linear activations stabilizes training and helps prevent overfitting. At the final stage, the model predicts the font class among the total number of categories present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFontCNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((8, 8))\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(256 * 8 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)               \n",
    "        x = x.view(x.size(0), -1)         \n",
    "        x = self.classifier(x)             \n",
    "        return x\n",
    "\n",
    "num_classes = df['font'].nunique()\n",
    "model = EnhancedFontCNN(num_classes=num_classes, in_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24041ad",
   "metadata": {},
   "source": [
    "Now that we have defined our model, we focus on training it effectively. The train_cnn function orchestrates the entire process, ensuring that learning is stable and optimized. We begin by configuring the optimizer and learning rate scheduler: Adam is our choice for optimization, set with a learning rate of 1e-3 and weight decay of 1e-4, helping prevent overfitting. To refine learning dynamics, we introduce CosineAnnealingLR, gradually decreasing the learning rate across 30 epochs, ensuring smoother convergence. Additionally, ReduceLROnPlateau detects stagnation and reduces the learning rate when validation loss stops improving, adapting the training flow dynamically.\n",
    "\n",
    "Since fonts appear with varying frequencies in the dataset, we address class imbalance by computing weights inversely proportional to their occurrences, these weights are integrated into CrossEntropyLoss, ensuring that rarer font classes influence training effectively. The weighting is derived from the dataset distribution using df['font'].value_counts(), allowing each font to contribute fairly to the learning process.\n",
    "\n",
    "During training, the model iterates through batches of images, using batch size = 32 and leveraging GPU acceleration if available. Images and labels are processed, predictions are generated, and loss is calculated using CrossEntropyLoss, while gradients are computed (loss.backward()) and applied (optimizer.step()), updating the model’s parameters to refine its accuracy. Throughout training, we track loss evolution and performance indicators to monitor improvements.\n",
    "\n",
    "Validation plays a crucial role in assessing model generalization, after each training phase, we switch to evaluation mode, disabling gradient computations. Here, we compute validation loss and accuracy, ensuring that unseen data is handled correctly and if validation accuracy improves, the model's weights are saved automatically (torch.save(model.state_dict(), 'best_model.pth')), preserving the best-performing version.\n",
    "\n",
    "To further enhance learning efficiency, we implement adaptive learning rate adjustments and early stopping, if validation loss plateaus for three consecutive epochs, ReduceLROnPlateau lowers the learning rate by half, encouraging further optimization. Meanwhile, if validation accuracy does not improve for five consecutive epochs, training stops early, preventing unnecessary resource consumption and reducing the risk of overfitting.\n",
    "\n",
    "After training is complete, we generate a classification report, summarizing model performance across different font categories, this provides clear insights into classification accuracy and helps fine-tune future improvements, by structuring the training process this way, we maintain a balance between efficiency and precision, ensuring that our model reaches optimal performance while adapting to dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, val_loader, df, device, epochs=30):\n",
    "    class_counts = df['font'].value_counts().sort_index()\n",
    "    class_weights = torch.tensor(1.0 / class_counts.values, dtype=torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=epochs,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    best_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    print(\"\\n🚀 Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            early_stop_counter = 0\n",
    "            print(f\"📦 Best model saved at epoch {epoch} with Val Acc {val_acc:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= 5:\n",
    "                print(\"⏸️ Early stopping\")\n",
    "                break\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        print(f\"\\nEpoch {epoch}/{epochs} — {epoch_time:.1f}s\")\n",
    "        print(f\" Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\" Val Acc: {val_acc:.4f} (Best: {best_acc:.4f}) | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    print(\"\\n Classification Report on validation set:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_counts.index.tolist()))\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7991f0",
   "metadata": {},
   "source": [
    "Now we initialize our model and begin training, ensuring that our setup aligns with the previously defined pipeline: First, we determine whether a GPU is available, selecting \"cuda\" if possible or defaulting to \"cpu\" otherwise, this ensures that computations are optimized for the available hardware; once the device is selected, we instantiate EnhancedFontCNN, specifying the number of font classes detected in the dataset. The model is moved to the chosen device, preparing it for efficient training. Printing the model architecture allows us to verify that its structure is correctly set up before proceeding.\n",
    "\n",
    "Training begins with the train_cnn function, where we pass the model, data loaders, dataset, device, and the number of epochs set to 10 and the training loader handles the main dataset, while the validation process utilizes the test loader, ensuring that the model evaluates performance on unseen data throughout the process.\n",
    "\n",
    "**Expected output:**\n",
    "As the training progresses over 10 epochs, we expect to see a gradual reduction in training loss and validation loss, while validation accuracy steadily increases and the learning rate (LR) remains stable unless adjusted by the scheduler.\n",
    "Typical output will look like this:\n",
    "- Each epoch will display the training time, loss values, accuracy, and best validation accuracy recorded;\n",
    "- Loss values should generally decrease as the model learns, though occasional fluctuations may occur;\n",
    "- Validation accuracy may vary initially but should improve over time, indicating better generalization;\n",
    "- When validation accuracy reaches a new peak, the best model weights will be saved;\n",
    "- Early stopping will trigger if the accuracy does not improve over consecutive epochs;\n",
    "\n",
    "At the end of training, the model evaluates on the validation set and produces a classification report, summarizing performance across font categories:\n",
    "- Precision: Measures how many predicted font labels were correct;\n",
    "- Recall: Reflects how well the model identified each font correctly;\n",
    "- F1-score: The harmonic mean of precision and recall, balancing both aspects;\n",
    "- Accuracy: Represents overall classification success across all samples.\n",
    "\n",
    "The macro average gives an equally weighted score across all font classes, while the weighted average accounts for varying class frequencies. Given this performance report, we can further refine the model based on misclassified fonts or adjust preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b497528",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n🔌 Device in use: {device}\")\n",
    "model = EnhancedFontCNN(num_classes=len(df['font'].unique())).to(device)\n",
    "print(model)\n",
    "\n",
    "trained_model, history = train_cnn(\n",
    "    model, \n",
    "    train_loader,    \n",
    "    val_loader,      \n",
    "    df,               \n",
    "    device,           \n",
    "    epochs=10         \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00d5c9",
   "metadata": {},
   "source": [
    "Now that our model has completed training, we assess its performance using a confusion matrix, which provides insights into classification accuracy across different font categories. This matrix visualizes how often predictions align with the actual labels and highlights misclassifications.\n",
    "We first gather predictions and ground-truth labels from the validation dataset, the model is set to evaluation mode, ensuring that computations remain stable and that gradients do not interfere with performance assessment. Using torch.no_grad(), we process validation batches, extract predictions, and store them for comparison.\n",
    "Once we have collected all labels and predictions, we compute the confusion matrix using confusion_matrix() that maps true labels against predicted labels, creating a structured representation of model performance and finally the results are then visualized using Seaborn’s heatmap, making it easier to interpret classification trends.\n",
    "\n",
    "**Expected Output**\n",
    "This confusion matrix visually represents the classification accuracy of a model, each cell contains the count of predictions for the corresponding true and predicted labels, ranging from \"augustus\" to \"vesta.\" Darker blue shades indicate higher frequencies, highlighting where the model performs well or struggles.\n",
    "- The highest count (21) occurs for \"laurel,\" suggesting strong model confidence in that category.\n",
    "- Some misclassifications occur, notably between \"ceres\" and \"juno,\" implying potential overlap or ambiguity in their features.\n",
    "- Certain categories, such as \"vesta,\" have lower counts, indicating either rarity in the dataset or challenges in distinguishing them.\n",
    "Overall, this confusion matrix serves as a crucial diagnostic tool, highlighting where improvements in feature engineering or model training may be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Collected {len(all_labels)} labels and predictions.\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(font_names))))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=font_names, yticklabels=font_names,\n",
    "            cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f019c7",
   "metadata": {},
   "source": [
    "# 2° Model : EnhancedFontCNN 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0711b",
   "metadata": {},
   "source": [
    "Now we look at improving our CNN training methodology, because the previous implementation worked, but there’s always room for optimization, so in this second version, we refine key aspects to enhance stability, efficiency, and learning adaptability.\n",
    "\n",
    "One of the main adjustments is the learning rate scheduling, in fact instead of relying solely on ReduceLROnPlateau, which adapts the learning rate based on validation stagnation, we incorporate CosineAnnealingLR to provide a smoother decay across epochs. This ensures that the learning rate decreases gradually, improving convergence while preventing abrupt changes, however, ReduceLROnPlateau is still retained as a backup mechanism when validation performance stops improving for multiple epochs.\n",
    "Additionally, we start training with a higher learning rate (1e-3) before gradually reducing it, as opposed to the previous fixed learning rate of 1e-4, this allows faster learning in early epochs while adapting more conservatively as training progresses.\n",
    "\n",
    "Early stopping logic has also been revised, the initial implementation stopped training after five epochs without improvement, while the newer version introduces an early stopping check after three epochs (this provides a quicker response but needs fine-tuning to ensure it doesn’t exit training prematurely).\n",
    "\n",
    "Finally, we streamline optimizer initialization to avoid redundant redefinitions: in the previous version, Adam was re-instantiated unnecessarily after each epoch, whereas now we ensure more consistent handling of optimization parameters across training cycles.\n",
    "By integrating these improvements, we aim for more stable and effective training while maintaining flexibility for further fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45de280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, val_loader, df, device, epochs=30):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    class_counts = df['font'].value_counts().sort_index()\n",
    "    class_weights = torch.tensor(1.0 / class_counts.values, dtype=torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    best_acc = 0.0\n",
    "    early_stop = 0\n",
    "\n",
    "    print(\"\\n🚀 Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            early_stop = 0\n",
    "            print(f\"📦 Best model saved at epoch {epoch} with Val Acc {val_acc:.4f}\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        print(f\"\\nEpoch {epoch}/{epochs} — {epoch_time:.1f}s\")\n",
    "        print(f\" Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\" Val Acc: {val_acc:.4f} (Best: {best_acc:.4f}) | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if early_stop >= 5:\n",
    "            print(\"⏸️ Early stopping\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n Classification Report on validation set:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_counts.index.tolist()))\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85542579",
   "metadata": {},
   "source": [
    "In this cell, we initialize EnhancedFontCNN, the improved version of our CNN model for font classification. Before starting training, we check for GPU availability and assign the model to the optimal device (cuda or cpu) to maximize performance, then the model is passed to the train_cnn() function, using the train_loader for training and the val_loader for validation. The training runs for 12 epochs, incorporating optimizations such as CosineAnnealingLR for smooth learning rate adjustment and early stopping strategies to prevent overfitting.\n",
    "This phase allows us to evaluate the model’s behavior before proceeding with further comparisons, such as fine-tuning a pretrained ResNet18 network. \n",
    "The expected output will be structured similarly to the previous CNN, displaying epoch progress, loss metrics, accuracy updates, and final validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n🔌 Device in uso: {device}\")\n",
    "model = EnhancedFontCNN(num_classes=len(df['font'].unique())).to(device)\n",
    "print(model)\n",
    "trained_model, history = train_cnn(\n",
    "    model, \n",
    "    train_loader,     \n",
    "    val_loader,      \n",
    "    df,               \n",
    "    device,           \n",
    "    epochs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad47cc",
   "metadata": {},
   "source": [
    "In this cell, we generate a confusion matrix to analyze the model's classification performance on the validation set. The matrix provides a visual representation of how well predictions align with actual labels, highlighting areas where misclassifications occur.\n",
    "We start by collecting all predicted and true labels from the validation set, ensuring that the model is in evaluation mode to prevent gradient updates, using torch.no_grad(), then we process validation batches, extract predictions, and store them for analysis.\n",
    "\n",
    "Once we have gathered predictions, we compute the confusion matrix using confusion_matrix() and visualize it with Seaborn’s heatmap. Each row corresponds to true labels, while columns represent predicted labels. Ideally, the highest values should appear along the diagonal, indicating correct classifications, while off-diagonal values reveal misclassifications, helping us identify patterns in errors.\n",
    "\n",
    "The expected output will be structured similarly to previous evaluations, showing a summary of label collection, the confusion matrix visualization, and any key misclassification trends, in particular:\n",
    "The EnhancedFontCNN model was trained for 12 epochs using a fixed learning rate of 1e-4 on CPU, here are some key observations:\n",
    "- Overall Performance:\n",
    "    - Final accuracy: 43.65% (Epoch 12); \n",
    "    - Best epoch: The last epoch (Epoch 12) achieved the highest validation accuracy (43.65%).\n",
    "    - Trend: Accuracy improved steadily throughout training, indicating the model is learning progressively.\n",
    "- Loss Analysis:\n",
    "    - Training loss: Gradually decreases from 2.37 to 1.66, showing consistent learning.\n",
    "    - Validation loss: Also decreases but with minor fluctuations, suggesting some data variability or potential for further regularization.\n",
    "- Class-Specific Performance\n",
    "\n",
    "*High-performing classes:*\n",
    "- Forum (Precision: 71%, Recall: 59%)\n",
    "- Aureus (Precision: 67%, Recall: 33%)\n",
    "- Laurel (Precision: 42%, Recall: 79%) → High recall, but prone to false positives.\n",
    "- Trajan (100% precision, but only 27% recall) → Very selective but identifies few samples correctly.\n",
    "\n",
    "*Underperforming classes:*\n",
    "- Vesta (Precision: 25%, Recall: 6%) → Difficult to classify.\n",
    "- Consul (Precision: 10%, Recall: 4%) → Could struggle with less distinctive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Collected {len(all_labels)} labels and predictions..\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(font_names))))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=font_names, yticklabels=font_names,\n",
    "            cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3aaf3a",
   "metadata": {},
   "source": [
    "# 3° Model : ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7848a85",
   "metadata": {},
   "source": [
    "After training our custom CNN models, we explored an alternative approach using a pretrained network. Instead of training from scratch, we leveraged ResNet18, a widely used architecture known for its efficiency in feature extraction. Since ResNet has already learned general visual patterns from a large dataset (ImageNet), we repurpose its existing weights while adapting the final layers to our specific task.\n",
    "\n",
    "To do this, we freeze all layers, preventing their weights from being updated during training. This retains the powerful feature extraction capabilities of ResNet while focusing learning efforts on the new classification layer. The original fully connected layer is replaced with a new one, designed to output predictions for our font dataset. Additionally, we apply dropout (0.5) to reduce overfitting, ensuring robust learning.\n",
    "\n",
    "This approach allows us to harness the strengths of a pretrained network while efficiently adapting it to our classification task.\n",
    "\n",
    "Let's start by adjusting the dataset transformations, specifically, the key changes we made are:\n",
    "- **RGB format instead of grayscale** : The model can now leverage color-based features instead of relying solely on texture and shape.  \n",
    "- **Different augmentation techniques** : Instead of geometric distortions (`RandomAffine`, `RandomPerspective`), the new version applies **color-based transformations** (`ColorJitter`) along with standard **rotation and flipping**.  \n",
    "- **Updated normalization values**: The new version uses ImageNet-style normalization (`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`), while the old version used grayscale normalization (`mean=[0.5], std=[0.5]`).  \n",
    "- **Simplified structure** : The new transformation setup is **directly defined** (`train_transforms`, `val_transforms`), making it more readable but less reusable compared to the function-based approach (`get_transforms()`).  \n",
    "\n",
    "**Impact of These Changes**:\n",
    "- The updated pipeline **preserves color details**, which may improve classification accuracy when color is relevant.  \n",
    "- The model now **focuses on visual variations rather than geometric distortions**, making transformations **less aggressive**.  \n",
    "- The simpler structure makes **modifications easier**, but sacrifices the flexibility of a function-based approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851500e4",
   "metadata": {},
   "source": [
    "In this setup, the pre-trained ResNet18 is used as the foundation, leveraging the vast knowledge it has gained from ImageNet to extract rich features from images. Instead of training the entire network from scratch, all of its existing layers are frozen, meaning their parameters remain unchanged, this ensures that the deep convolutional layers continue functioning as powerful feature detectors, while the final classification layer is specifically adapted to the new task—identifying different fonts.\n",
    "\n",
    "To achieve this, the original fully connected (fc) layer at the end of ResNet is replaced with a custom classification head that consists of a dropout mechanism, which helps prevent overfitting by randomly disabling some neurons during training, ensuring better generalization and that is followed by a linear layer that maps the extracted features to the correct number of font classes, allowing the model to make precise predictions.\n",
    "\n",
    "Looking at the architecture, ResNet18 begins with a large 7×7 convolutional layer, which captures broad features in the image; followed by batch normalization, ensuring stable training, and a max-pooling layer, reducing dimensionality efficiently. The network then consists of multiple residual blocks, which introduce skip connections—crucial for maintaining strong gradients during backpropagation, blocks that systematically increase the number of channels, moving from 64 to 512, refining features as they progress.\n",
    "\n",
    "Each stage contains stacked convolutional layers with ReLU activations and batch normalization, so as the depth increases, some layers also incorporate downsampling, reducing the spatial resolution while increasing the representational power of the network. Toward the end, an adaptive average pooling layer ensures that the feature maps are compact enough for classification, regardless of the input size.\n",
    "\n",
    "Finally, the modified classifier head ensures that the extracted features are properly categorized into the required font classes, with the original backbone frozen, training focuses entirely on this new section of the network, making the process both efficient and effective.\n",
    "\n",
    "By making these modifications, the model retains the powerful feature extraction capabilities of ResNet18, while also being fine-tuned to the specific task of font recognition. This approach balances speed, efficiency, and accuracy, making it an excellent choice when working with a well-defined classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, len(font_names))\n",
    ")\n",
    "model = resnet.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb48823",
   "metadata": {},
   "source": [
    "This code cell defines a standard training and evaluation loop for our classification model using PyTorch, the goal here was to train the model on our dataset over a number of epochs and monitor its performance on both the training and validation sets. The training loop begins by setting the model in training mode, and for each batch, it performs the typical sequence of steps: zeroing the gradients, running the forward pass, computing the loss using cross-entropy (which is well suited for multi-class classification problems), backpropagating the error, and updating the weights using the Adam optimizer. We only optimized the final fully connected layer (`model.fc`) since we were using a pre-trained backbone and focusing on fine-tuning only the classifier head. After each epoch, we compute the average training loss and accuracy over the entire training set.\n",
    "\n",
    "Once the training phase is done for an epoch, the model is evaluated on the validation set and at this point, the model is switched to evaluation mode to disable dropout and batch normalization updates, and we wrap the validation loop in a `torch.no_grad()` context to avoid computing gradients. Again, we calculate loss and accuracy, this time for the validation data, to assess generalization.\n",
    "\n",
    "In theory, this code would output the training and validation loss and accuracy for each epoch, giving us a clear idea of how well the model is learning and whether it's overfitting or underfitting. However, in practice, we couldn't run this loop until the end because the model we were working with—most likely a version of ResNet—turned out to be too computationally heavy for our available hardware, so, despite the logic being correct and well-structured, we couldn't complete the training process and observe the expected performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bd285",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    # Validation Loop\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "    print(f\"Validation  Loss: {val_loss:.4f} | Validation  Acc: {val_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2356a4",
   "metadata": {},
   "source": [
    "# 4° Model : MobileNetV2 (CPU friendly version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d80d3e",
   "metadata": {},
   "source": [
    "The previous model — based on ResNet18 — showed some early promise but ultimately didn't perform as expected for the font classification task. While its deep residual architecture is powerful and widely used, it didn’t yield satisfactory generalization in our case, likely due to the domain-specific nuances in font data that require more efficient spatial encoding and perhaps better regularization.\n",
    "\n",
    "To address these limitations, we transitioned to a new setup using **MobileNetV2** as the backbone. This model, as shown in the code below, is designed with efficiency and speed in mind, making it a strong candidate for tasks where we seek a balance between performance and computational cost.\n",
    "\n",
    "The core of the architecture is a pre-trained MobileNetV2 model, from which we use only the `features` part — a deep stack of lightweight convolutional layers composed primarily of **inverted residual blocks** and **depthwise separable convolutions**. These architectural choices reduce the number of parameters significantly while preserving the network’s ability to learn rich hierarchical features from input images. The pre-trained weights come from training on ImageNet, and we freeze these layers to retain their powerful general-purpose visual representations without retraining them from scratch.\n",
    "\n",
    "To adapt the model to our specific classification task, we introduce a new classification head. It consists of:\n",
    "- An **adaptive average pooling** layer, which ensures that feature maps from the backbone are globally averaged, regardless of the input image size.\n",
    "- A **dropout layer** (with 40% probability), added to prevent overfitting by randomly deactivating neurons during training, encouraging robustness.\n",
    "- A final **fully connected linear layer** that takes the flattened features (dimension 1280, which is the final output size of MobileNetV2) and maps them directly to the number of font classes.\n",
    "\n",
    "In parallel to modifying the architecture, we also redefined the image transformation pipeline to better suit this network and the variability in font data. First, all input images are resized and center-cropped to a uniform size of **224×224 pixels**, ensuring compatibility with the expected input of MobileNetV2. For training data specifically, we include augmentations such as **random horizontal flipping**, **small-angle rotations**, and **slight color jittering**. These augmentations serve a critical purpose: they artificially introduce variability into the training set, helping the model generalize better and avoid overfitting to fixed font shapes or orientations. Finally, all images are normalized using ImageNet's mean and standard deviation values, aligning them with the scale and distribution the pre-trained backbone expects.\n",
    "\n",
    "The training setup also includes class weighting in the loss function to address class imbalance, the use of the `AdamW` optimizer (which combines adaptive learning rate with decoupled weight decay), and a `ReduceLROnPlateau` scheduler that dynamically lowers the learning rate when the validation loss plateaus.\n",
    "\n",
    "**Output:**\n",
    "Now, looking at the training output, we observe steady and consistent progress over the 5 epochs. Initially, in **Epoch 1**, the model starts with a training accuracy of approximately **22.7%** and a validation accuracy of **42.1%**, indicating that while the classifier has just started learning, the pre-trained backbone already provides meaningful features.\n",
    "\n",
    "As training proceeds, both training and validation metrics improve significantly:\n",
    "- By **Epoch 3**, validation accuracy reaches **51.2%**, suggesting that the classifier is beginning to generalize well.\n",
    "- By **Epoch 5**, training accuracy increases to **47.4%**, and validation accuracy reaches **58.3%**, demonstrating a solid upward trajectory in performance.\n",
    "\n",
    "The validation loss consistently decreases from **1.77** to **1.24**, which further confirms that the model is not just memorizing training data but is genuinely improving its ability to discriminate between different font classes. Importantly, after each epoch, the scheduler evaluates the validation loss to determine if learning rate adjustments are needed. The best-performing model — in terms of validation loss — is saved to `\"best_tl_mobilenetv2.pth\"`, ensuring that we retain the most generalizable weights.\n",
    "\n",
    "In summary, switching to MobileNetV2 proved to be a more effective approach for this task. By combining a frozen, efficient feature extractor with a compact classifier head and a carefully adapted transformation strategy, we achieved better generalization, faster training, and overall more reliable performance on the font classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97432f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_tl(image_size=(224,224), train=True):\n",
    "    base = [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "    ]\n",
    "    aug = []\n",
    "    if train:\n",
    "        aug = [\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        ]\n",
    "    norm = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ]\n",
    "    return transforms.Compose(base + aug + norm)\n",
    "\n",
    "tl_train_ds = FontDataset(df.iloc[train_idx].reset_index(drop=True),\n",
    "                          img_dir,\n",
    "                          transform=get_transforms_tl((224,224), train=True))\n",
    "tl_val_ds   = FontDataset(df.iloc[test_idx].reset_index(drop=True),\n",
    "                          img_dir,\n",
    "                          transform=get_transforms_tl((224,224), train=False))\n",
    "\n",
    "class TLMobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = models.mobilenet_v2(pretrained=True).features\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        in_f = 1280 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_f, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "tl_model     = TLMobileNetV2(num_classes=len(font_to_label),\n",
    "                             freeze_backbone=True).to('cpu')\n",
    "tl_criterion = nn.CrossEntropyLoss(weight=class_weights.to('cpu'))\n",
    "tl_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, tl_model.parameters()),\n",
    "                            lr=5e-4, weight_decay=1e-4)\n",
    "tl_scheduler = optim.lr_scheduler.ReduceLROnPlateau(tl_optimizer,\n",
    "                                                   mode='min',\n",
    "                                                   patience=2,\n",
    "                                                   factor=0.5)\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, 6):\n",
    "    # Train\n",
    "    tl_model.train()\n",
    "    train_loss, train_acc = 0.0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        tl_optimizer.zero_grad()\n",
    "        out = tl_model(imgs)\n",
    "        loss = tl_criterion(out, labels)\n",
    "        loss.backward()\n",
    "        tl_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        train_acc  += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc  /= len(train_loader.dataset)\n",
    "\n",
    "    # Val\n",
    "    tl_model.eval()\n",
    "    val_loss, val_acc = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            out = tl_model(imgs)\n",
    "            loss = tl_criterion(out, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            val_acc  += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc  /= len(val_loader.dataset)\n",
    "\n",
    "    tl_scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"[TL-MobileNetV2] Epoch {epoch} | \"\n",
    "          f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
    "          f\"Val   loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(tl_model.state_dict(), \"best_tl_mobilenetv2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84604a",
   "metadata": {},
   "source": [
    "# 5° Model : MobileNetV2 no augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bd91d",
   "metadata": {},
   "source": [
    "In this experiment, we begin by defining the image transformation pipeline in the get_transforms() function that returns two identical pipelines—one for training and one for validation—since no data augmentation is applied here. Each image is resized to 224×224 pixels to fit MobileNetV2’s input requirements and converted from grayscale to three-channel format and this is essential because MobileNetV2 was trained on RGB ImageNet images, and expects three-channel inputs.\n",
    "\n",
    "The images are then normalized using the standard ImageNet statistics (mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), step that is critical in ensuring the compatibility of our input data with the pre-trained weights used in the network, and allows us to leverage transfer learning effectively.\n",
    "\n",
    "We wrap the image data into a custom FontDataset class, feed it to PyTorch DataLoaders and we use a batch size of 32, enabling shuffling for the training set to promote better generalization, while keeping the test set in fixed order for reproducibility.\n",
    "\n",
    "Next, we load the pre-trained MobileNetV2 and freeze all convolutional layers in the features block, this allows us to reuse the generic visual features learned from ImageNet, such as edge and texture detectors, without retraining them and we then replace the final classification layer with a new linear layer mapping the feature vector to our specific number of font classes (num_classes).\n",
    "\n",
    "For the training procedure, we use CrossEntropyLoss, suitable for multi-class classification problems; the optimizer of choice is Adam, which adapts learning rates internally for faster convergence. In addition, a ReduceLROnPlateau scheduler monitors the validation accuracy and automatically reduces the learning rate when performance stops improving—an essential feature for escaping local minima and stabilizing learning.\n",
    "\n",
    "The training loop is fully custom so in each epoch, the model alternates between a training phase, where it learns from labeled data and updates weights, and a validation phase, where its performance is evaluated on unseen data without updating gradients. We compute both training and validation accuracy at every epoch, and whenever a new best validation accuracy is achieved, we save the model to disk as a checkpoint (best_mobilenetv2.pth).\n",
    "\n",
    "**Output:**\n",
    "\n",
    "The output tells the story of a training process that steadily gained ground both in reducing loss and improving accuracy, on both the training and validation sets:\n",
    "\n",
    "At the start, as is often the case, the loss is quite high (around 67) and accuracy is roughly 29%, so the model essentially begins from scratch, which is expected for a non-trivial problem. However, already after the first epoch, we see a significant jump: the loss drops to about 47.9 and accuracy more than doubles, reaching 51.5%. Validation follows a similar trend with 40% accuracy, indicating a decent level of generalization right from the beginning.\n",
    "\n",
    "Subsequent epochs show a steady progression, without sudden leaps but with consistent improvement, we notice the loss gradually decreases to just below 29 by the final epoch, while training accuracy climbs to over 71%, meanwhile, validation maintains solid performance, peaking at 71.43% accuracy in epoch 8, when the best model checkpoint is saved.\n",
    "\n",
    "It’s normal that validation accuracy doesn’t perfectly mirror training accuracy every epoch — for example, at epoch 3 validation dips slightly compared to epoch 2, this reflects natural fluctuations due to data variability and the complexity of the task and this doesn’t indicate overfitting; overall, the model keeps improving.\n",
    "\n",
    "Saving the model every time validation accuracy reaches a new high is a smart move because it preserves the most effective version of the model without losing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874678de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "train_dataset = FontDataset(train_df, IMG_DIR, transform=train_transform)\n",
    "test_dataset = FontDataset(test_df, IMG_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Using device:\", device)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "for param in mobilenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.classifier[1].in_features, num_classes)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet.classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "def train_model(model, train_loader, test_loader, epochs=10):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"📚 Epoch {epoch+1}: Loss = {running_loss:.4f} | Accuracy = {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"🔎 Validation Accuracy: {val_acc:.2f}%\")\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_mobilenetv2.pth\")\n",
    "            print(\"✅ Saved best model\")\n",
    "\n",
    "    print(f\"\\n Best validation accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915bbd7",
   "metadata": {},
   "source": [
    "Here we print the model's metrics and confusion matrix, in particular:\n",
    "The model achieves an overall accuracy of 72% across 252 test images — a solid result, especially considering no heavy augmentation or ensemble methods are involved.\n",
    "Here's how the model performs across the 11 font classes:\n",
    "- forum and consul absolutely dominate with near-perfect performance — forum has an F1-score of 0.97 and consul 0.94. This suggests that these fonts have highly distinguishable features, and the model has learned them with confidence.\n",
    "- aureus and cicero also perform very well (F1 around 0.81–0.82), with balanced precision and recall, meaning the model identifies them correctly and consistently.\n",
    "- colosseum is interesting: it has the lowest precision (0.47) but a surprisingly high recall (0.70). That means the model predicts \"colosseum\" too often — including when it shouldn’t — but does manage to catch most true colosseum examples (it's a classic case of over-prediction on a class that may share visual features with others).\n",
    "- roman stands out as problematic: it has decent precision (0.82), but a very low recall (0.35), indicating that the model rarely predicts \"roman\" even when it’s the correct label — possibly due to its features being too similar to more dominant classes like \"consul\" or \"trajan.\"\n",
    "- laurel, vesta, and trajan have middling scores, with F1s hovering around 0.56 to 0.70, these are likely more ambiguous classes in terms of visual features.\n",
    "\n",
    "Overall, the macro average (simple average across all classes) and weighted average (which considers class imbalance) are both around 0.72–0.75, confirming that performance is not being dominated by just a few well-performing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_font = {v: k for k, v in font_to_label.items()}\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "mobilenet.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[label_to_font[i] for i in sorted(set(y_true))]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[label_to_font[i] for i in sorted(set(y_true))],\n",
    "            yticklabels=[label_to_font[i] for i in sorted(set(y_true))])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - MobileNetV2 (No Augmentation)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09b1f8",
   "metadata": {},
   "source": [
    "# 6° Model: MobileNetV2 with augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c4f05",
   "metadata": {},
   "source": [
    "In this cell we find the same model as before, but with data augmentation and so in particular the difference is that it begins by defining the `get_transforms()` function, where the training transformations are more complex than a simple resize and normalize; in fatc we resize images to 224x224 pixels, but then apply a series of augmentations designed to increase the diversity of training samples, these include random horizontal flips (with a 50% chance), small random rotations up to ±10 degrees, and random changes in brightness and contrast. These augmentations help the model generalize better by simulating realistic variations that fonts might exhibit in scanned or photographed documents.\n",
    "\n",
    "For what concerns the results:\n",
    "The output shows the training and validation metrics over seven epochs of our MobileNetV2 fine-tuning process:\n",
    "\n",
    "- Right from the start, we observe a clear downward trend in the training loss, starting from a high 68.04 in epoch 1 down to around 40 by epoch 7, this drop in loss indicates the model is effectively learning to minimize its prediction errors on the training data. \n",
    "\n",
    "- Accuracy on the training set shows a steady and consistent increase — from a modest 27.9% in the first epoch to nearly 57% by the seventh epoch, this rise confirms that the model’s predictions are becoming increasingly accurate on the data it has seen during training.\n",
    "\n",
    "- What’s particularly encouraging is the behavior of the validation accuracy, which rises even faster than the training accuracy in the early epochs, jumping from 38.1% in the first epoch to nearly 62% by epoch 7, this suggests that the model is not just memorizing training samples, but genuinely learning features that generalize to unseen data.\n",
    "\n",
    "- Each time the validation accuracy improves beyond the previous best, the model’s weights are saved, as indicated by the “✅ Saved best model” messages, this ensures that even if subsequent training causes some overfitting or accuracy dips, we retain the best version of the model discovered so far.\n",
    "\n",
    "- The relative closeness between training and validation accuracy, especially in the later epochs, also hints that overfitting is being controlled well — likely thanks to the frozen convolutional layers, data augmentation, and learning rate scheduling.\n",
    "\n",
    "In summary, this output tells a positive story of effective learning: the model quickly improves its ability to classify fonts with consistent gains in both training and validation accuracy, while steadily lowering its loss, setting a solid foundation for further refinement or deployment.\n",
    "\n",
    "**The output:**\n",
    "\n",
    "The output shows the training and validation metrics over seven epochs of our MobileNetV2 fine-tuning process.\n",
    "\n",
    "Right from the start, we observe a clear downward trend in the training loss, starting from a high 68.04 in epoch 1 down to around 40 by epoch 7, this drop in loss indicates the model is effectively learning to minimize its prediction errors on the training data. \n",
    "\n",
    "Accuracy on the training set shows a steady and consistent increase — from a modest 27.9% in the first epoch to nearly 57% by the seventh epoch, this rise confirms that the model’s predictions are becoming increasingly accurate on the data it has seen during training.\n",
    "\n",
    "What’s particularly encouraging is the behavior of the validation accuracy, which rises even faster than the training accuracy in the early epochs, jumping from 38.1% in the first epoch to nearly 62% by epoch 7, that suggests that the model is not just memorizing training samples, but genuinely learning features that generalize to unseen data.\n",
    "\n",
    "Each time the validation accuracy improves beyond the previous best, the model’s weights are saved, as indicated by the “✅ Saved best model” messages ensuring that even if subsequent training causes some overfitting or accuracy dips, we retain the best version of the model discovered so far.\n",
    "\n",
    "The relative closeness between training and validation accuracy, especially in the later epochs, also hints that overfitting is being controlled well — likely thanks to the frozen convolutional layers, data augmentation, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "\n",
    "train_dataset = FontDataset(train_df, IMG_DIR, transform=train_transform)\n",
    "test_dataset = FontDataset(test_df, IMG_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = df['label'].nunique()\n",
    "\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "for param in mobilenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.classifier[1].in_features, num_classes)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet.classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, verbose=True)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=10):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {running_loss:.4f} | Accuracy = {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_mobilenetv2.pth\")\n",
    "            print(\"✅ Saved best model.\")\n",
    "\n",
    "    print(f\"\\n Best validation accuracy: {best_acc:.2f}%\")\n",
    "train_model(mobilenet, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be799a7",
   "metadata": {},
   "source": [
    "# 7° Model : MobileNetV2 Partial Fine Tuned - the final one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338788e5",
   "metadata": {},
   "source": [
    "After evaluating the previous ResNet-based architecture and realizing its limitations in capturing font-specific features—possibly due to the overly deep structure or insufficient specialization for fine-grained textural differences—we decided to switch to a lighter and more efficient model: MobileNetV2. This change is particularly motivated by the need for faster training, fewer parameters to tune, and a better balance between accuracy and computational cost.\n",
    "\n",
    "As shown in the code, we also redefined the image preprocessing pipeline to align better with the nature of this new architecture and the task at hand. The transformation function `get_transforms_tl` starts by resizing all input images to a fixed shape of 224×224 pixels, ensuring compatibility with MobileNetV2's expected input size. A `CenterCrop` is applied to remove unnecessary background noise, especially helpful if the dataset contains inconsistencies in how fonts are positioned within images.\n",
    "\n",
    "When training is enabled, we inject controlled randomness into the dataset using data augmentation techniques. A `RandomHorizontalFlip` with a 50% chance introduces left-right symmetry, which might help the model generalize better to fonts with mirrored features. `RandomRotation(10)` slightly rotates images, simulating natural distortions that might occur in font usage. Finally, `ColorJitter` with small variations in brightness, contrast, saturation, and hue helps the model avoid overfitting to specific lighting or rendering conditions.\n",
    "\n",
    "Regardless of whether we are in training or validation mode, the final stage includes a conversion to tensor and normalization. We adopt the standard ImageNet normalization statistics—mean = `[0.485, 0.456, 0.406]` and std = `[0.229, 0.224, 0.225]`—which matches the distribution of the data MobileNetV2 was pre-trained on. This ensures a smoother transition and compatibility between the learned filters and the new input domain.\n",
    "\n",
    "Regarding the model itself, `TLMobileNetV2` loads a pre-trained version of MobileNetV2 and fine-tunes only the last two layers of its convolutional backbone—specifically the ones indexed as \"18\" and \"19\". These layers capture higher-level abstractions, and by allowing only them to update, we strike a balance between preserving powerful pre-trained features and tailoring the model to our font classification task.\n",
    "\n",
    "The classifier head is then replaced: instead of the original dense layer, we introduce a `Dropout(0.4)` regularizer followed by a new `Linear` layer that maps the extracted feature vector to the correct number of font classes. This minimalistic yet effective head is designed to prevent overfitting and keep the number of trainable parameters low.\n",
    "\n",
    "Training uses `CrossEntropyLoss` combined with class weights to handle potential class imbalance, and the optimizer of choice is `AdamW`, which includes weight decay to further promote generalization. A `ReduceLROnPlateau` scheduler dynamically adjusts the learning rate when the validation loss plateaus, making the training process more adaptive and robust.\n",
    "\n",
    "We also leverage mixed precision training via `torch.amp` and a `GradScaler`, which speeds up computation on GPU while maintaining stability in gradient updates. Early stopping with a patience of 5 epochs ensures we avoid overfitting by halting training when no further improvement is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a676fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_tl(image_size=(224,224), train=True):\n",
    "    base = [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "    ]\n",
    "    if train:\n",
    "        aug = [\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "        ]\n",
    "    else:\n",
    "        aug = []\n",
    "    norm = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ]\n",
    "    return transforms.Compose(base + aug + norm)\n",
    "\n",
    "tl_train_ds = FontDataset(df.iloc[train_idx].reset_index(drop=True),\n",
    "                          img_dir,\n",
    "                          transform=get_transforms_tl((224,224), train=True))\n",
    "tl_val_ds   = FontDataset(df.iloc[test_idx].reset_index(drop=True),\n",
    "                          img_dir,\n",
    "                          transform=get_transforms_tl((224,224), train=False))\n",
    "\n",
    "tl_train_loader = DataLoader(tl_train_ds,\n",
    "                             batch_size=32,\n",
    "                             shuffle=True,\n",
    "                             num_workers=0,\n",
    "                             pin_memory=True)\n",
    "tl_val_loader = DataLoader(tl_val_ds,\n",
    "                           batch_size=32,\n",
    "                           shuffle=False,\n",
    "                           num_workers=0,\n",
    "                           pin_memory=True)\n",
    "\n",
    "class TLMobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=True):\n",
    "        super().__init__()\n",
    "        self.backbone = mobilenet_v2(pretrained=True)\n",
    "        if fine_tune:\n",
    "            for name, param in self.backbone.features.named_parameters():\n",
    "                if \"18\" in name or \"19\" in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        in_f = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_f, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "tl_model = TLMobileNetV2(num_classes=len(font_to_label), fine_tune=True).to(device)\n",
    "\n",
    "tl_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "tl_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, tl_model.parameters()),\n",
    "                           lr=5e-4, weight_decay=1e-4)\n",
    "tl_scheduler = optim.lr_scheduler.ReduceLROnPlateau(tl_optimizer,\n",
    "                                                    mode='min',\n",
    "                                                    patience=3,\n",
    "                                                    factor=0.5)\n",
    "tl_scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "best_val = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # TRAINING\n",
    "    tl_model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    with tqdm(tl_train_loader, unit=\"batch\") as train_bar:\n",
    "        for imgs, labels in train_bar:\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "    \n",
    "    train_bar = tqdm(tl_train_loader, \n",
    "                    desc=f\"Epoch {epoch}/{n_epochs} [Train]\", \n",
    "                    leave=False)\n",
    "    \n",
    "    for imgs, labels in train_bar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        tl_optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=torch.cuda.is_available()):  \n",
    "            out = tl_model(imgs)\n",
    "            loss = tl_criterion(out, labels)\n",
    "\n",
    "        \n",
    "        tl_scaler.scale(loss).backward()\n",
    "        tl_scaler.step(tl_optimizer)\n",
    "        tl_scaler.update()\n",
    "    \n",
    "    train_loss /= len(tl_train_loader.dataset)\n",
    "    train_acc /= len(tl_train_loader.dataset)\n",
    "\n",
    "    # VAL\n",
    "    tl_model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    val_bar = tqdm(tl_val_loader, \n",
    "                  desc=f\"Epoch {epoch}/{n_epochs} [Val]\", \n",
    "                  leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_bar:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = tl_model(imgs)\n",
    "            loss = tl_criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            val_acc += (out.argmax(1) == labels).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix({\n",
    "                'val_loss': f\"{loss.item():.4f}\",\n",
    "                'val_acc': f\"{(out.argmax(1) == labels).float().mean().item():.4f}\"\n",
    "            })\n",
    "\n",
    "    val_loss /= len(tl_val_loader.dataset)\n",
    "    val_acc /= len(tl_val_loader.dataset)\n",
    "\n",
    "    tl_scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}/{n_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | \"\n",
    "          f\"LR: {tl_optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(tl_model.state_dict(), \"best_tl_mobilenetv2.pth\")\n",
    "        print(\"💾 Miglior modello salvato!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acc40d",
   "metadata": {},
   "source": [
    "Once training is completed, we move to the final evaluation phase to assess how well the model generalizes to unseen data. To do this, we load the weights corresponding to the best validation performance and set the model in evaluation mode — ensuring deterministic behavior by disabling mechanisms like dropout.\n",
    "\n",
    "We then iterate over the entire validation set, performing inference on each image and recording both predicted and true labels. This allows us to compute aggregate metrics like precision, recall, and F1-score for each class. To complement these metrics, we generate a confusion matrix — a powerful visual tool that shows exactly where the model is making mistakes and which classes are most frequently confused.\n",
    "\n",
    "**Results Analysis**\n",
    "The model achieves an overall accuracy of 73%, a significant improvement over the results obtained with ResNet18. But the real insight lies in the per-class breakdown:\n",
    "- Strongly classified fonts include consul and forum, with impressively high F1-scores (above 0.90). This suggests the model has learned to distinguish these fonts with high confidence and consistency.\n",
    "- Fonts like aureus, cicero, vesta, and trajan also show good balance between precision and recall, meaning the learned representations are both reliable and generalizable.\n",
    "- Some fonts remain challenging. Notably, augustus and roman exhibit high precision but low recall, this means the model is conservative in predicting these classes — it only does so when very confident — but often fails to detect them, misclassifying them as other fonts. For instance, roman has a recall of only 31%, indicating frequent false negatives. Colosseum and laurel land in a more mediocre range, with F1-scores around 0.50–0.60, this may be due to visual similarity to other font styles.\n",
    "\n",
    "Both macro and weighted averages hover around 0.70, reflecting a reasonably balanced overall performance with room for improvement on the more ambiguous classes. The confusion matrix reinforces this, showing specific class-level confusions — likely tied to shared typographic features — that might benefit from targeted data augmentation or more discriminative loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10087d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Final evaluation on the Validation Set\")\n",
    "tl_model.load_state_dict(torch.load(\"best_tl_mobilenetv2.pth\"))\n",
    "tl_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Barra di avanzamento per la valutazione\n",
    "eval_bar = tqdm(tl_val_loader, desc=\"Evaluating\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in eval_bar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        out = tl_model(imgs)\n",
    "        preds = out.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=font_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=font_names, yticklabels=font_names, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Validation Set\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701add98",
   "metadata": {},
   "source": [
    "In the cel below we are just selecting the best model till now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dca456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.load_state_dict(torch.load(\"best_tl_mobilenetv2.pth\"))\n",
    "tl_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b48461",
   "metadata": {},
   "source": [
    "And here we are printing the results, so that we can see the results in terms of confusion matrix and metrics, that we have already examinated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tl_val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        out = tl_model(imgs)\n",
    "        preds = out.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "print(\"\\n CLASSIFICATION REPORT\")\n",
    "print(classification_report(all_labels, all_preds, target_names=font_names))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=font_names, yticklabels=font_names, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a63a7e",
   "metadata": {},
   "source": [
    "Finally in the code below we decided to create a function that prints a 3x3 matrix of sample images and shows us in practice the correct font and the font classified by our model. We wanted to include this, to give a more practical and even more effective from the visual point of view of the result obtained. So for each image it shows:\n",
    "- T: the true label (i.e., the correct font label),\n",
    "- P: The predicted (i.e., model prediction)\n",
    "and we can see that in this case we get 6 right predictions out of 9, in particular we wrong forum which is classified as augustus, laurel as cicero and roman as laurel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(dataset, model, device, font_names, num_images=9):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 10))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(dataset)):\n",
    "        img, true_label = dataset[i]\n",
    "        with torch.no_grad():\n",
    "            output = model(img.unsqueeze(0).to(device))\n",
    "            pred_label = output.argmax(1).item()\n",
    "\n",
    "        ax = axs[count]\n",
    "        img_disp = img.permute(1, 2, 0).numpy()\n",
    "        img_disp = img_disp * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # un-normalize\n",
    "        img_disp = img_disp.clip(0, 1)\n",
    "\n",
    "        ax.imshow(img_disp)\n",
    "        ax.set_title(f\"T: {font_names[true_label]}\\nP: {font_names[pred_label]}\", fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esempi dal validation set\n",
    "plot_predictions(tl_val_ds, tl_model, device, font_names, num_images=9)z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d5e19",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
